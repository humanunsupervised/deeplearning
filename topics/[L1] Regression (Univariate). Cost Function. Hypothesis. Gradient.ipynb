{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [L1] Regression (Univariate). Cost Function. Hypothesis. Gradient.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This is the first post in a series, covering notes and key topics in Andrew Ng's seminal course on Machine Learning from Standford University, the web's most highly rated machine learning course, and content direct from one of the field's most influential contributers. \n",
    "\n",
    "The series is a compilation of notes from my time through the course, and is in essence aimed to be a useful machine learning handbook that students can refer to or that practioners can use as a reference for foundational review.\n",
    "\n",
    "Machine Learning from Standford takes practioners through a bottom-up approach introduction into the field, covering the theory and the math which underpins modern machine learning today. Topics include linear regression, classification, neural networks, and also practical information for working with machine learning models in the wild and what to look for during training.\n",
    "\n",
    "It comes highly recommended. https://www.coursera.org/learn/machine-learning\n",
    "\n",
    "---\n",
    "\n",
    "This first post covers the basics. An introduction into what machine learning actually is, and the topics of model hypothesis, cost functions, and gradient descent.\n",
    "\n",
    "\n",
    "## What is Machine Learning\n",
    "\n",
    "\n",
    "#### Definitions\n",
    "\n",
    "ML Definition - Field of study that gives computers the ability to learn without being explicitly programmed.\n",
    "\n",
    "ML Definition - A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/01.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "What is Machine Learning?\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/03.png' style=\"float:left\" width=\"600px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Supervised Learning\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/04.png' style=\"float:left\" width=\"600px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Unsupervised Learning\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/05.png' style=\"float:left\" width=\"600px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Note: A good rule of thumb is that supervised learning teaches machines to make predictions, while unsupervised learning teaches machines to find relationships.\n",
    "\n",
    "#### Classification and Regression\n",
    "\n",
    "1. Classification - Discrete Valued Output (ie is this email spam or not spam)\n",
    "\n",
    "2. Regression - Continuous Valued Output / Real valued output\n",
    "\n",
    "Linear Regression - Data is said to \"regress\" to a mean. Predicts continuous valued output.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/02.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "Hypothesis. Univariate\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/06.png' style=\"float:left\" width=\"300px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Hypothesis in Linear Regression\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/07.png' style=\"float:left\" width=\"350px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Another Example:\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/08.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "Cost Function - Average of differences between predicted values and the actual values. Also known as the \"squared error\".\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/09.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/10.png' style=\"float:left\" width=\"300px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Cost function - measures the accuracy of our hypothesis function by using this.\n",
    "\n",
    "Minimising the cost of the prediction is the goal of accurate machine learning models.\n",
    "\n",
    "### Additional Notes\n",
    "\n",
    "Why does the cost function include multiplying by 1/(2m)?\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/11.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "In the cost function, why don't we use the absolute value, instead of the mean squared?\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/12.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "### Cost Function Derivative\n",
    "\n",
    "To prepare for the next step in gradient descent, we need to understand the derivative of the cost function.\n",
    "\n",
    "Starting with the cost function, we can work out its derivative like this. Which leads to the following definition of the derivative of the cost function.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/13.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Derivative of the Cost function.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/14.png' style=\"float:left\" width=\"200px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "The general form for gradient decent.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/15.png' style=\"float:left\" width=\"350px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Gradient Descent is a way to minimise the cost function. We use the derivative of the cost function in the gradient descent algorithm.\n",
    "\n",
    "Gradient descent formula for linear regression. We take theta, then subtract the derivative of the cost function that is also multiplies by a tuned alpha learning rate. We repeat this until convergence.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/16.png' style=\"float:left\" width=\"350px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "General formula for Gradient Descent using the derivative notation. Where j represents the feature index number. For univariate Linear regression, j is either 0 or 1.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/17.png' style=\"float:left\" width=\"350px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "So for example, the gradient descent component for theta1 looks like this.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/18.png' style=\"float:left\" width=\"350px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Gradient Descent. Visualised - Used to minimise the cost function.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/19.png' style=\"float:left\" width=\"350px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "\n",
    "### Gradient Descent - Derivative\n",
    "\n",
    "This component here of the gradient descent formula represents the derivative of the cost function. The first for theta0, the second for theta1.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/20.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "\n",
    "#### Gradient Descent - Convergence Intuition.\n",
    "\n",
    "1. Gradient Descent uses the derivative of the cost function J\n",
    "2. The intuition behind gradient descent convergence is that the derivative of the cost function will approach 0 near the local minimum.\n",
    "3. At the minimum, Gradient descent will always be 0 (ie a flat slope line with no gradient)\n",
    "4. Gradient Descent can converge to a local minimum even with a fixed learning rate. So no need to change the learning rate of time.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/24.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "5. If the Gradient descent is already at local minimum, another step will just keep the descent at the local minimum\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/25.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "6. If the learning rate alpha is too small, gradient descent can be slow.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/26.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "7. If learning rate alpha is too large, it may not converge at all, or even diverge.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/27.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "#### Cost Function and Gradient Descent - Visualised Intuitions\n",
    "\n",
    "Cost Function Plotted in three dimensional space with theta0 and theta1. The bottom of the trough is still the minimised point of the cost function.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/28.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Gradient Descent. Visualised - Used to minimise the cost function\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/29.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "#### Cost Function - Contour Plot Intuitions\n",
    "\n",
    "Hypothesis and Cost Function - Using Contour Plots\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/30.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Contour Plot Intuition 1\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/31.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Contour Plot Intuition 2\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/32.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## ML Model Approach - Summary of Process\n",
    "\n",
    "### Overview\n",
    "\n",
    "1. Hypothesis \n",
    "2. Cost Function\n",
    "3. Minimising the cost (Gradient Descent).\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/21.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "#### Gradient Descent Steps\n",
    "\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/22.png' style=\"float:left\" width=\"300px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "General formula for Gradient Descent summarised.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/23.png' style=\"float:left\" width=\"300px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "The initial values (in Cost functions where there are more than one local minima) - the initial values of gradient descent can affect which local minima you end up with\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
