{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Univariate Regression. Cost Function. Hypothesis.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## What is Machine Learning\n",
    "\n",
    "1. Classification - Discrete Valued Output\n",
    "\n",
    "2. Regression - Continuous Valued Output / Real valued output\n",
    "\n",
    "Supervised learning\n",
    "\n",
    "Unsupervised Learning\n",
    "\n",
    "A good rule of thumb is that supervised learning teaches machines to make predictions, while unsupervised learning teaches machines to find relationships.\n",
    "\n",
    "ML Definition - Field of study that gives computers the ability to learn without being explicitly programmed.\n",
    "\n",
    "\n",
    "ML Definition - A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/01.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Linear Regression - Data is said to \"regress\" to a mean. Predicts continuous valued output.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/02.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "What is Machine Learning?\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/03.png' style=\"float:left\" width=\"600px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Supervised Learning\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/04.png' style=\"float:left\" width=\"600px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Unsupervised Learning\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/05.png' style=\"float:left\" width=\"600px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "---\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "Hypothesis. Univariate\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/06.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Linear Regression\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/07.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Example\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/08.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "Cost Function - Average of differences between predicted values and the actual values. Also known as the \"squared error\".\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/09.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/10.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Cost function - measures the accuracy of our hypothesis function by using this.\n",
    "\n",
    "Minimising the cost of the prediction is the goal of accurate machine learning models.\n",
    "\n",
    "### Additional Notes\n",
    "\n",
    "Why does the cost function include multiplying by 1/(2m)?\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/11.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "In the cost function, why don't we use the absolute value, instead of the mean squared?\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/12.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "### Cost Function Derivative\n",
    "\n",
    "To prepare for the next step in gradient descent, we need to understand the derivative of the cost function.\n",
    "\n",
    "Starting with the cost function, we can work out its derivative like this. Which leads to the following definition of the derivative of the cost function.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/13.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Derivative of the Cost function.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/14.png' style=\"float:left\" width=\"200px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "The general form for gradient decent.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/15.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Gradient Descent is a way to minimise the cost function. We use the derivative of the cost function in the gradient descent algorithm.\n",
    "\n",
    "Gradient descent formula for linear regression. We take theta, then subtract the derivative of the cost function that is also multiplies by a tuned alpha learning rate. We repeat this until convergence.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/16.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "General formula for Gradient Descent using the derivative notation. Where j represents the feature index number. For univariate Linear regression, j is either 0 or 1.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/17.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "So for example, the gradient descent component for theta1 looks like this.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/18.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Gradient Descent. Visualised - Used to minimise the cost function.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/19.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "### Gradient Descent - Derivative\n",
    "\n",
    "This component here of the gradient descent formula represents the derivative of the cost function. The first for theta0, the second for theta1.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/20.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Linear Regression - Summary of Process\n",
    "\n",
    "Hypothesis, Cost Function, and minimising the cost.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/21.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Gradient Descent - the process.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/22.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "General formula for Gradient Descent summarised.\n",
    "<br clear=\"both\"/>\n",
    "<img src='images/lesson1/23.png' style=\"float:left\" width=\"400px\"/>\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "The initial values (in Cost functions where there are more than one local minima) - the initial values of gradient descent can affect which local minima you end up with\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
